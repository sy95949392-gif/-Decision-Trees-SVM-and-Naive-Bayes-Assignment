{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Classification: Decision Trees, SVM, and Naive Bayes| Assignment"
      ],
      "metadata": {
        "id": "_lt56x1HEk2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1.  What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "-->Information Gain (IG) measures how much uncertainty (or impurity) is reduced after splitting a dataset on a particular feature.\n",
        "\n",
        "In simple words:\n",
        "\n",
        "üëâ It tells us which feature gives the most ‚Äúinformation‚Äù about the target variable.\n",
        "\n",
        "The feature with the highest Information Gain is chosen to split the node in a decision tree.\n",
        "\n",
        "How Information Gain is Used in Decision Trees\n",
        "\n",
        "Step-by-step:\n",
        "\n",
        "1.Calculate entropy of the parent node\n",
        "\n",
        "2.Split the data using a feature\n",
        "\n",
        "3.Calculate entropy of each child node\n",
        "\n",
        "4.Compute weighted average entropy\n",
        "\n",
        "5.Subtract from parent entropy\n",
        "\n",
        "6.Choose the feature with highest IG\n",
        "\n",
        "That feature becomes the decision node.\n",
        "\n",
        "Simple Example\n",
        "\n",
        "Imagine a dataset for Play Tennis (Yes / No).\n",
        "\n",
        "Entropy before split = 0.94\n",
        "\n",
        "Split by Outlook\n",
        "\n",
        "Entropy after split = 0.69\n"
      ],
      "metadata": {
        "id": "9MWeRSSQEk5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "-->\n",
        "\n",
        "## Difference Between Gini Impurity and Entropy\n",
        "\n",
        "Both **Gini Impurity** and **Entropy** are metrics used in **decision trees** to measure how impure (mixed) a dataset is and to choose the best feature for splitting.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Gini Impurity\n",
        "\n",
        "**Definition:**\n",
        "Gini Impurity measures the **probability of misclassifying** a randomly chosen data point if it were labeled according to the class distribution in the node.\n",
        "\n",
        "**Formula:**\n",
        "[\n",
        "Gini = 1 - \\sum p_i^2\n",
        "]\n",
        "\n",
        "### Strengths\n",
        "\n",
        "* Computationally **faster** (no logarithms)\n",
        "* Works well with **large datasets**\n",
        "* Tends to isolate the **most frequent class quickly**\n",
        "\n",
        "### Weaknesses\n",
        "\n",
        "* Slightly less sensitive to changes in class probabilities\n",
        "* Less interpretable from an information-theory perspective\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "* Used in **CART algorithm**\n",
        "* Default choice in **scikit-learn**\n",
        "* Preferred when performance and speed matter\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Entropy\n",
        "\n",
        "**Definition:**\n",
        "Entropy measures the **amount of uncertainty or randomness** in the dataset using concepts from information theory.\n",
        "\n",
        "**Formula:**\n",
        "[\n",
        "Entropy = -\\sum p_i \\log_2(p_i)\n",
        "]\n",
        "\n",
        "### Strengths\n",
        "\n",
        "* More **theoretically grounded**\n",
        "* More sensitive to small changes in class distribution\n",
        "* Produces slightly more **balanced trees**\n",
        "\n",
        "### Weaknesses\n",
        "\n",
        "* Computationally **slower** (log calculations)\n",
        "* Can be biased toward features with many unique values\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "* Used in **ID3 and C4.5 algorithms**\n",
        "* Preferred in **academic, theoretical, and exam settings**\n",
        "* Useful when interpretability matters\n",
        "\n",
        "---\n",
        "\n",
        "## Side-by-Side Comparison\n",
        "\n",
        "| Aspect             | Gini Impurity                         | Entropy                   |\n",
        "| ------------------ | ------------------------------------- | ------------------------- |\n",
        "| Measures           | Misclassification probability         | Uncertainty / information |\n",
        "| Formula complexity | Simple                                | More complex              |\n",
        "| Speed              | Faster                                | Slower                    |\n",
        "| Sensitivity        | Lower                                 | Higher                    |\n",
        "| Tree structure     | Slightly biased toward dominant class | More balanced             |\n",
        "| Algorithms         | CART                                  | ID3, C4.5                 |\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaway\n",
        "\n",
        "> **Gini Impurity is faster and preferred in practice, while Entropy is more informative and preferred for theoretical understanding. Both usually lead to similar decision tree splits.**\n",
        "\n",
        "---\n",
        "\n",
        "### Perfect One-Line Answer (Exam Ready)\n",
        "\n",
        "**Gini Impurity measures misclassification probability and is faster, while Entropy measures uncertainty using information theory and is more sensitive; both are used to select optimal splits in decision trees.**\n",
        "\n",
        "If you want, I can also add:\n",
        "\n",
        "* A **numerical example**\n",
        "* **Python code** comparison\n",
        "* A **diagram-based explanation**\n"
      ],
      "metadata": {
        "id": "3XM4MCW2Ek8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "40ec13Q2Ek-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "-->\n",
        "## What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "**Pre-Pruning** (also called **early stopping**) is a technique used to **stop the growth of a decision tree early**‚Äî*before* it becomes too complex.\n",
        "\n",
        "üëâ The goal is to **prevent overfitting** by limiting how much the tree can grow.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Do We Need Pre-Pruning?\n",
        "\n",
        "A fully grown decision tree:\n",
        "\n",
        "* Fits training data **too well**\n",
        "* Learns **noise**\n",
        "* Performs poorly on **new/unseen data**\n",
        "\n",
        "Pre-pruning avoids this by stopping splits that are not useful enough.\n",
        "\n",
        "---\n",
        "\n",
        "## How Pre-Pruning Works\n",
        "\n",
        "During tree construction, a split is **not allowed** if it violates predefined conditions.\n",
        "\n",
        "### Common Pre-Pruning Criteria\n",
        "\n",
        "1. **Maximum depth**\n",
        "\n",
        "   * Stop splitting once a certain tree depth is reached\n",
        "     *(e.g., `max_depth = 5`)*\n",
        "\n",
        "2. **Minimum samples per node**\n",
        "\n",
        "   * Do not split if a node has fewer than *k* samples\n",
        "     *(e.g., `min_samples_split = 20`)*\n",
        "\n",
        "3. **Minimum samples in a leaf**\n",
        "\n",
        "   * Ensures leaves have enough data\n",
        "     *(e.g., `min_samples_leaf = 10`)*\n",
        "\n",
        "4. **Minimum impurity decrease**\n",
        "\n",
        "   * Split only if impurity reduction exceeds a threshold\n",
        "\n",
        "5. **Statistical significance tests**\n",
        "\n",
        "   * Split only if improvement is statistically meaningful\n",
        "\n",
        "---\n",
        "\n",
        "## Advantages of Pre-Pruning\n",
        "\n",
        "‚úî Reduces **overfitting**\n",
        "‚úî Faster training\n",
        "‚úî Produces **simpler, more interpretable trees**\n",
        "‚úî Uses less memory\n",
        "\n",
        "---\n",
        "\n",
        "## Disadvantages of Pre-Pruning\n",
        "\n",
        "‚úò Risk of **underfitting**\n",
        "‚úò May stop splits that are actually important\n",
        "‚úò Requires careful tuning of hyperparameters\n",
        "\n",
        "---\n",
        "\n",
        "## Pre-Pruning vs Post-Pruning (Quick Contrast)\n",
        "\n",
        "| Aspect       | Pre-Pruning        | Post-Pruning             |\n",
        "| ------------ | ------------------ | ------------------------ |\n",
        "| When applied | During tree growth | After full tree is built |\n",
        "| Approach     | Early stopping     | Cut back branches        |\n",
        "| Risk         | Underfitting       | Higher computation       |\n",
        "| Speed        | Faster             | Slower but more accurate |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4EUC0_2iElB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data          # features\n",
        "y = data.target        # target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Decision Tree Classifier using Gini Impurity\n",
        "dt = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(feature_names, dt.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "vafKiq-EIQmB",
        "outputId": "6699227f-e452-4a3b-e9ee-ca051e601fdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "-->\n",
        "\n",
        "## What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification and regression** that works by finding the **optimal decision boundary (hyperplane)** that best separates data points of different classes.\n",
        "\n",
        "---\n",
        "\n",
        "## Core Idea (Intuition)\n",
        "\n",
        "üëâ SVM doesn‚Äôt just separate classes ‚Äî it separates them **as far apart as possible**.\n",
        "\n",
        "* The decision boundary is called a **hyperplane**\n",
        "* The closest data points to the hyperplane are called **support vectors**\n",
        "* The distance between the hyperplane and the support vectors is called the **margin**\n",
        "* SVM tries to **maximize this margin**\n",
        "\n",
        "A larger margin = better generalization.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components of SVM\n",
        "\n",
        "### 1. Hyperplane\n",
        "\n",
        "A line (2D), plane (3D), or higher-dimensional boundary that separates classes.\n",
        "\n",
        "### 2. Support Vectors\n",
        "\n",
        "Data points closest to the hyperplane that **define its position**.\n",
        "\n",
        "### 3. Margin\n",
        "\n",
        "The maximum distance between the hyperplane and the nearest data points from each class.\n",
        "\n",
        "---\n",
        "\n",
        "## Handling Non-Linear Data (Kernel Trick)\n",
        "\n",
        "When data is not linearly separable, SVM uses **kernels** to map data into higher dimensions.\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "* **Linear**\n",
        "* **Polynomial**\n",
        "* **RBF (Gaussian)** ‚Äî most popular\n",
        "* **Sigmoid**\n",
        "\n",
        "This is called the **kernel trick**.\n",
        "\n",
        "---\n",
        "\n",
        "## Advantages of SVM\n",
        "\n",
        "‚úî Works well with **high-dimensional data**\n",
        "‚úî Effective when number of features > samples\n",
        "‚úî Robust to overfitting (with proper kernel & parameters)\n",
        "\n",
        "---\n",
        "\n",
        "## Disadvantages of SVM\n",
        "\n",
        "‚úò Computationally expensive for large datasets\n",
        "‚úò Sensitive to **kernel choice** and hyperparameters\n",
        "‚úò Less interpretable compared to decision trees\n"
      ],
      "metadata": {
        "id": "qolT6BR8ElH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6:  What is the Kernel Trick in SVM?\n",
        "\n",
        "-->\n",
        "## What is the Kernel Trick in SVM?\n",
        "\n",
        "The **Kernel Trick** is a technique used in **Support Vector Machines (SVMs)** to handle **non-linearly separable data** by implicitly mapping input data into a **higher-dimensional feature space**, where a **linear separation becomes possible**, **without explicitly computing that transformation**.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Do We Need the Kernel Trick?\n",
        "\n",
        "Some datasets cannot be separated by a straight line (or plane).\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "* Circles, spirals, or XOR-type patterns\n",
        "\n",
        "Instead of manually transforming features, SVM uses a kernel function to **compute inner products in higher dimensions efficiently**.\n",
        "\n",
        "---\n",
        "\n",
        "## How the Kernel Trick Works (Intuition)\n",
        "\n",
        "* Original space ‚Üí data is **non-linear**\n",
        "* Higher-dimensional space ‚Üí data becomes **linearly separable**\n",
        "* Kernel function calculates similarity **as if** data were mapped to that higher space\n",
        "\n",
        "‚ú® No actual transformation is computed ‚Äî that‚Äôs the ‚Äútrick‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "## Common Kernel Functions\n",
        "\n",
        "1. **Linear Kernel**\n",
        "   [\n",
        "   K(x, x') = x \\cdot x'\n",
        "   ]\n",
        "   Used when data is already linearly separable.\n",
        "\n",
        "2. **Polynomial Kernel**\n",
        "   [\n",
        "   K(x, x') = (x \\cdot x' + c)^d\n",
        "   ]\n",
        "   Captures polynomial relationships.\n",
        "\n",
        "3. **RBF (Gaussian) Kernel**\n",
        "   [\n",
        "   K(x, x') = \\exp(-\\gamma ||x - x'||^2)\n",
        "   ]\n",
        "   Most widely used; handles complex boundaries.\n",
        "\n",
        "4. **Sigmoid Kernel**\n",
        "   [\n",
        "   K(x, x') = \\tanh(\\alpha x \\cdot x' + c)\n",
        "   ]\n",
        "   Inspired by neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## Advantages of the Kernel Trick\n",
        "\n",
        "‚úî Enables SVM to solve **non-linear problems**\n",
        "‚úî Avoids expensive computations in high dimensions\n",
        "‚úî Highly flexible with different kernel choices\n",
        "\n",
        "---\n",
        "\n",
        "## Limitations\n",
        "\n",
        "‚úò Kernel and parameter selection is crucial\n",
        "‚úò Can be slow for very large datasets\n",
        "‚úò Risk of overfitting with complex kernels\n"
      ],
      "metadata": {
        "id": "iccG88hLElKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7:  Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "#kernels on the Wine dataset, then compare their accuracies.\n",
        "#Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "#on the same dataset.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracy results\n",
        "print(\"Accuracy with Linear Kernel SVM:\", linear_accuracy)\n",
        "print(\"Accuracy with RBF Kernel SVM:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "id": "oN3q55OlKCbP",
        "outputId": "0d496641-aebb-49c0-e40d-dd0d40e6c173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel SVM: 0.9814814814814815\n",
            "Accuracy with RBF Kernel SVM: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "-->\n",
        "\n",
        "## What is the Na√Øve Bayes Classifier?\n",
        "\n",
        "The **Na√Øve Bayes classifier** is a **supervised probabilistic machine learning algorithm** based on **Bayes‚Äô Theorem**, used mainly for **classification tasks** such as text classification, spam detection, and sentiment analysis.\n",
        "\n",
        "It predicts the class that has the **highest posterior probability** given the input features.\n",
        "\n",
        "### Bayes‚Äô Theorem:\n",
        "\n",
        "[\n",
        "P(C|X) = \\frac{P(X|C),P(C)}{P(X)}\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "## Why Is It Called ‚ÄúNa√Øve‚Äù?\n",
        "\n",
        "It is called **‚Äúna√Øve‚Äù** because it makes a **strong simplifying assumption**:\n",
        "\n",
        "> üëâ **All features are conditionally independent given the class label.**\n",
        "\n",
        "This assumption is usually **not true in real-world data**, but it simplifies calculations a lot ‚Äî and surprisingly, the model still works very well in practice.\n",
        "\n",
        "---\n",
        "\n",
        "## Intuition (Simple Example)\n",
        "\n",
        "For spam detection:\n",
        "\n",
        "* Words like *‚Äúfree‚Äù* and *‚Äúwin‚Äù* often appear together\n",
        "* Na√Øve Bayes **assumes they are independent**\n",
        "* Even with this unrealistic assumption, it still classifies spam accurately\n",
        "\n",
        "That‚Äôs the ‚Äúna√Øve‚Äù part.\n",
        "\n",
        "---\n",
        "\n",
        "## Types of Na√Øve Bayes\n",
        "\n",
        "1. **Gaussian Na√Øve Bayes** ‚Äì continuous features\n",
        "2. **Multinomial Na√Øve Bayes** ‚Äì text and word counts\n",
        "3. **Bernoulli Na√Øve Bayes** ‚Äì binary features\n",
        "\n",
        "---\n",
        "\n",
        "## Advantages\n",
        "\n",
        "‚úî Very fast and memory efficient\n",
        "‚úî Works well with **high-dimensional data**\n",
        "‚úî Performs especially well in **text classification**\n",
        "\n",
        "---\n",
        "\n",
        "## Limitations\n",
        "\n",
        "‚úò Independence assumption is often violated\n",
        "‚úò Cannot model feature interactions well\n",
        "‚úò Probability estimates can be inaccurate\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wqoo9FZ2ElOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "-->\n",
        "## Differences Between Gaussian, Multinomial, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "All three are variants of the **Na√Øve Bayes classifier**, differing mainly in the **type of data they assume** and how they model feature distributions.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Gaussian Na√Øve Bayes\n",
        "\n",
        "### Assumption\n",
        "\n",
        "* Features are **continuous**\n",
        "* Values follow a **normal (Gaussian) distribution**\n",
        "\n",
        "### Probability Model\n",
        "\n",
        "[\n",
        "P(x|C) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
        "]\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "* Medical data\n",
        "* Sensor measurements\n",
        "* Any dataset with real-valued features\n",
        "\n",
        "### Example\n",
        "\n",
        "* Height, weight, temperature\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Multinomial Na√Øve Bayes\n",
        "\n",
        "### Assumption\n",
        "\n",
        "* Features represent **counts or frequencies**\n",
        "* Data follows a **multinomial distribution**\n",
        "\n",
        "### Key Characteristics\n",
        "\n",
        "* Works with **non-negative integer values**\n",
        "* Considers **frequency of features**\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "* Text classification\n",
        "* Spam detection\n",
        "* Document categorization\n",
        "\n",
        "### Example\n",
        "\n",
        "* Word counts in documents (Bag-of-Words, TF)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Bernoulli Na√Øve Bayes\n",
        "\n",
        "### Assumption\n",
        "\n",
        "* Features are **binary (0 or 1)**\n",
        "* Presence or absence of a feature matters\n",
        "\n",
        "### Key Characteristics\n",
        "\n",
        "* Penalizes **absence of a feature**\n",
        "* Suitable for binary vectors\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "* Binary text features\n",
        "* Yes/No or True/False attributes\n",
        "\n",
        "### Example\n",
        "\n",
        "* Whether a word appears in a document or not\n",
        "\n",
        "---\n",
        "\n",
        "## Side-by-Side Comparison\n",
        "\n",
        "| Aspect         | Gaussian NB     | Multinomial NB        | Bernoulli NB  |\n",
        "| -------------- | --------------- | --------------------- | ------------- |\n",
        "| Feature type   | Continuous      | Discrete counts       | Binary        |\n",
        "| Distribution   | Gaussian        | Multinomial           | Bernoulli     |\n",
        "| Values allowed | Any real number | Non-negative integers | 0 or 1        |\n",
        "| Best for       | Numeric data    | Word counts           | Word presence |\n",
        "| Text data      | ‚ùå               | ‚úÖ                     | ‚úÖ             |\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaway (Exam Gold)\n",
        "\n",
        "> **Gaussian NB is used for continuous data, Multinomial NB for count-based data, and Bernoulli NB for binary features, all under the Na√Øve independence assumption.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GZVgeooQK-ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10:  Breast Cancer Dataset\n",
        "#Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
        "#dataset and evaluate accuracy.\n",
        "#Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "#sklearn.datasets.\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Na√Øve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Na√Øve Bayes:\", accuracy)\n"
      ],
      "metadata": {
        "id": "QLZizKJPL2-j",
        "outputId": "261223db-f3ef-43d1-ecef-6c4917e2de73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Na√Øve Bayes: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}